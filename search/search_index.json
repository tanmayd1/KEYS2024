{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#introduction","title":"Introduction","text":"<p> <p>Author: Tanmay Dewangan </p> <p>Advisor: Nirav Merchant </p>"},{"location":"#about","title":"About","text":"<p>Tanmay Dewangan is a rising senior at Paradise Valley High School CREST Computer Science. He is interning at the KEYS Internship program at the University of Arizona during the summer of 2024.</p> <p>This website follows the FAIR and CARE data principles and hopes to help further open science. </p>"},{"location":"#main-project","title":"Main Project","text":"<p>I am creating a system that will make it easier for researchers to store, manage, and share large datasets in a way that is efficient, reliable, and compliant with federal data-sharing requirements. </p> <p>Currently, researchers often face fragmented and inefficient methods for handling data, which can lead to difficulties in data discovery, accessibility, and collaboration.  These challenges necessitate improvements to promote more effective data sharing, which in turn can enhance research outcomes and reproducibility.  Not addressing these issues can result in missed opportunities, slower progress, and non-compliance with increasingly stringent data-sharing policies. </p> <p>My approach involves using CKAN to create a centralized catalog of datasets stored in the CyVerse data commons, which will enhance metadata management and searchability.  Stakeholders include researchers who will benefit from streamlined data management and sharing, as well as institutions that need to comply with data-sharing mandates.  Success in this project will lead to easier and more productive research, improved infrastructure capabilities, and alignment with open science principles, benefitting related disciplines and society at large by making research more transparent and reproducible. </p> <p>Risks include potential technical challenges but this can be mitigated through careful planning, thorough testing, and the advisement of Dr. Merchant and other lab mentors.  The project is expected to take about 6 weeks to complete.  Success can be assessed through milestones such as the successful integration of CKAN with CyVerse, user adoption rates, and compliance with data-sharing policies.  Completion will be determined when the system is fully functional, widely adopted, and proven to enhance research productivity and compliance.</p>"},{"location":"#results","title":"Results","text":"<p>[Results]</p>"},{"location":"#daily-logs","title":"Daily Logs","text":"<p>Link to the logbook page</p>"},{"location":"acknowledgments/","title":"Acknowledgments","text":""},{"location":"acknowledgments/#acknowledgments","title":"Acknowledgments","text":"<p>[I am grateful to...]</p> <p>[This material relied upon the work of ...]</p> <p>[My Internship was funded by BIO5 Institute KEYS]</p>"},{"location":"assignment1/","title":"Assignment 1: Internship Description","text":"<p>I am working under Dr. Nirav Merchant for the CyVerse lab which provides scientists with powerful platforms to handle huge datasets and complex analyses. For this project, I will be focusing on the CyVerse data commons, which are collaborative spaces or platforms designed to store, manage, and share data, typically for research purposes. I will use CKAN (Comprehensive Knowledge Archive Network) to catalog the datasets that are stored in the CyVerse data commons in order to provide a centralized, searchable, and metadata-rich repository that enhances data discoverability, accessibility, and sharing among researchers, while ensuring compliance with federal data-sharing requirements. This project is crucial as it provides an integrated solution for managing, sharing, and accessing large-scale research datasets in a compliant and efficient manner, thereby enhancing collaboration, transparency, and reproducibility in scientific research. Creating tailored data commons supports the principles of open science and addresses the need for scalable data infrastructure in research.</p>"},{"location":"assignment2/","title":"Assignment 2: Introduction to your Research","text":""},{"location":"assignment2/#purpose-prior-research","title":"Purpose &amp; Prior Research:","text":"<p>The purpose of this research is to create a new method to help scientists share and combine their research data easily and securely using a data commons. A data commons is a platform that allows researchers to store, share, and analyze large datasets in a secure and organized manner. This will make it easier for scientists to work together and make new discoveries. Previous research has demonstrated the importance of data commons in collaborative research environments, such as the NIH's Data Commons Pilot Phase Consortium, which highlighted the need for standardized protocols to manage and share data effectively (National Institutes of Health). Similarly, studies by the National Academies of Sciences, Engineering, and Medicine (2018) have emphasized the critical role of data sharing in advancing scientific discovery. This project aims to build on these ideas by making a simple and flexible system for data sharing.</p>"},{"location":"assignment2/#need-problem-statement","title":"Need &amp; Problem Statement:","text":"<p>This study is important because it aims to simplify the complex process of data sharing and integration which is often a bottleneck in research. By developing a standardized protocol, we expect to enhance the efficiency and effectiveness of data management which will lead to more reproducible research outcomes. Our hypothesis is that a novel data engineering protocol can significantly improve the creation and customization of data commons. We aim to test this by developing and validating a protocol that researchers can use to create their own tailored data commons.</p>"},{"location":"assignment2/#references","title":"References","text":"<p>National Academies of Sciences, Engineering, and Medicine. (2018). Open science by design. In National Academies Press eBooks. https://doi.org/10.17226/25116</p> <p>National Institutes of Health. (n.d.). NIH Data Commons Pilot Phase. https://commonfund.nih.gov/commons</p>"},{"location":"assignment3/","title":"Assignment 3: Materials and Methods","text":"<p>In this project, I am developing a robust Python script to automate the migration of datasets from CyVerse\u2019s Discovery Environment to the CKAN data management platform. This involves using the Discovery Environment API to retrieve dataset metadata (like creation dates, author, description, DOI identifier, etc) and the CKAN API to register these datasets within CKAN. This approach was chosen for its ability to seamlessly integrate these two systems and it made sure that datasets were accurately and efficiently transferred without manual intervention. The metadata that is being transferred is very important for maintaining the integrity and originality of datasets during migration. </p> <p>The Python programming language was chosen for its versatility and the availability of robust libraries for handling HTTP requests and JSON data manipulation. The integration of these APIs and Python allows for automated and scheduled transfers, which reduces the risk of human error and ensures consistency in the dataset registration process.</p>"},{"location":"assignment4/","title":"Assignment 4: Results","text":"<p>My data will be presented as an image depicting a user interface which will allow researchers to utilize my Python script that I developed. In addition to this, there may be graphs showing an improvement in the efficiency and accuracy of dataset migrations to CKAN; however, this has not yet been confirmed by my lab. These results will allow viewers to see first hand how my project has enhanced the ability of researchers to access and share datasets.</p>"},{"location":"assignment5/","title":"Assignment 5: Long Abstract","text":"<p>In the field of scientific research, efficient data management is crucial for effective collaboration, data sharing, and accessibility. This project focuses on enhancing dataset management for researchers by migrating datasets to a more accessible platform called CKAN while also standardizing metadata format and enhancing the storage space of datasets. Firstly, to migrate datasets, I created a program in Python, a programming language, which allowed me to access datasets and their metadata from the current platform and then register those same datasets into CKAN. It was important to perform this migration to CKAN because the previous platform was not accessible to the general public. Secondly, in order to enhance the efficiency of datasets, I created another Python script to take CSV (Comma Separated Values) files in datasets and convert them into something called Parquet files which require less storage space while maintaining accuracy. These Parquet files can then be put into CKAN. Another aspect of my project is converting datasets into something called the Croissant metadata format. This metadata format is a common way of formatting metadata which will make it easier for other researchers to utilize these datasets by improving data interoperability. The last part of my project is creating a user interface which would allow researchers to easily use my python scripts to manage datasets. This is essentially the results of my datasets because it integrates all of my python scripts and functionality into one place. </p>"},{"location":"assignment6/","title":"Assignment 6 Title and Short Abstract","text":""},{"location":"assignment6/#title-here","title":"Title Here","text":"<p>Streamlining Metadata Management and Dataset Migration</p>"},{"location":"assignment6/#short-abstract-here","title":"Short Abstract Here","text":"<p>Efficient data management is vital for scientific research collaboration, sharing, and accessibility. This project enhances dataset management by migrating datasets to a more accessible platform, standardizing metadata, and improving storage efficiency. First, I created a program to access and migrate datasets, making them publicly accessible. Second, another program converts certain files into a different file format which requires less storage space while maintaining accuracy. Additionally, the project includes converting datasets to different metadata formats, improving data interoperability. Finally, I developed a user interface to allow researchers to easily manage datasets, integrating all functionalities into one accessible platform. Future work should address the limitations related to this project such as conducting extensive testing and validation as well as allowing for the use of more data platforms.</p>"},{"location":"bag_it_notes/","title":"BagIt Notes","text":"<p>Note: These notes are from Day 17 (6/26/24) of the logbook.</p>"},{"location":"bag_it_notes/#bagit-overview","title":"BagIt Overview","text":"<ul> <li>A hierarchical file packaging format designed for the reliable transfer and storage of digital content</li> <li>Provides a standardized way to package files and metadata into bags for transfer and storage</li> <li>Each file in a BagIt package is accompanied by a checksum. <ul> <li>Ensures that files can be validated for integrity upon receipt or retrieval, detecting any corruption or alteration.</li> </ul> </li> <li>BagIt packages are designed to be both human-readable and machine-readable, making them accessible and easy to use</li> <li>Bags can include metadata files that describe the contents and context of the data<ul> <li>useful for data management and preservation</li> </ul> </li> </ul>"},{"location":"bag_it_notes/#bagit-structure","title":"BagIt Structure","text":"<ol> <li>Bagit.txt<ul> <li>Contains information about the bag itself, such as the version of the BagIt specification used and character encoding]</li> </ul> </li> <li>Manifest files<ul> <li>Lists of files in the bag and their corresponding checksums</li> <li>Used to verify the integrity of the files in the bag</li> </ul> </li> <li>Data Directory<ul> <li>Contains the actual files being packaged</li> </ul> </li> <li>Tag files<ul> <li>Additional metadata files that provide information about the bag and its contents</li> <li>Can include information such as provenance, rights, and preservation metadata</li> </ul> </li> </ol> <p>Example Directory Structure:</p> <pre><code>my_bag/\n\u251c\u2500\u2500 bagit.txt\n\u251c\u2500\u2500 bag-info.txt\n\u251c\u2500\u2500 manifest-md5.txt\n\u2514\u2500\u2500 data/\n    \u251c\u2500\u2500 file1.txt\n    \u251c\u2500\u2500 file2.jpg\n    \u2514\u2500\u2500 subdirectory/\n        \u2514\u2500\u2500 file3.pdf\n</code></pre>"},{"location":"ckan_api_testing_prompts/","title":"CKAN API Testing Prompts","text":"<p>Note: These prompts are from Day 7 (6/11/24) of the logbook.</p> <p>System Instructions:</p> <p>Same as the instructions for Example 1 in practice.</p>"},{"location":"ckan_api_testing_prompts/#helping-set-up-the-ckan-api-and-creating-a-dataset","title":"Helping set up the CKAN API and Creating a Dataset","text":"<p>Prompt:</p> <p>Create a python script that sets up the CKAN API and creates a dataset in CKAN in a specific organization.  Make sure to include comments in the script to explain what each part of the code does.</p> <p>Response:</p> <pre><code># Import the necessary libraries\nimport requests\nimport json\n\n# Set the URL for the CKAN API\nckan_url = 'https://data.yourorganization.org'\n\n# Set the API key for the CKAN API\napi_key = 'YOUR_API_KEY'\n\n# Function to create a new dataset in a specific organization\ndef create_dataset_in_org(name, title, description, owner_org):\n    # Set the URL for the CKAN API endpoint to create a new dataset\n    url = f'{ckan_url}/api/3/action/package_create'\n\n    # Set the headers for the request\n    headers = {\n        'Authorization': api_key,\n        'Content-Type': 'application/json'\n    }\n    # Prepare the data to be sent in the request\n    data = {\n        'name': name,\n        'title': title,\n        'notes': description,\n        'owner_org': owner_org\n    }\n    # Make the request to create the dataset\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    return response.json()\n</code></pre>"},{"location":"ckan_api_testing_prompts/#creating-a-function-to-get-the-dataset-id","title":"Creating a Function to Get the Dataset ID","text":"<p>Prompt:</p> <p>Create a Python function that takes a dataset name as input and returns the dataset ID from CKAN.  The function should handle cases where the dataset is not found and return <code>None</code> in such cases.</p> <p>Response:</p> <pre><code>def get_dataset_id(dataset_name):\n    url = f'{ckan_url}/api/3/action/package_show'\n    headers = {'Authorization': api_key}\n    params = {'id': dataset_name}\n\n    response = requests.get(url, headers=headers, params=params)\n\n    if response.status_code == 200:\n        # The dataset exists and the response contains the dataset metadata\n        dataset_metadata = response.json()\n        if dataset_metadata['success']:\n            return dataset_metadata['result']['id']\n        else:\n            print(f\"Error: {dataset_metadata['error']['message']}\")\n            return None\n    elif response.status_code == 404:\n        print(f\"Dataset '{dataset_name}' not found.\")\n        return None\n    else:\n        print(f\"An error occurred: {response.status_code} - {response.text}\")\n        return None\n</code></pre>"},{"location":"ckan_api_testing_prompts/#debugging-the-get_dataset_id-function","title":"Debugging the get_dataset_id Function","text":"<p>Prompt:</p> <p>Can you tell me why this function call is saying that the dataset is not found even though I know that the dataset name is correct and the dataset exists in the organization?</p> <p>(Same function as above)</p> <p>Response:</p> <p>If the function call is saying that the dataset is not found even though the dataset name is correct and it exists, there might be several potential issues to investigate:</p> <ol> <li>Dataset Name: Ensure that the dataset_name provided is exactly as it appears in CKAN, including case sensitivity and any special characters.</li> <li>Authorization: Verify that the API key has the necessary permissions to access the dataset.</li> <li>CKAN Instance URL: Ensure that the ckan_url is correct and points to the right CKAN instance.</li> <li>API Endpoint: Ensure that the CKAN instance's API endpoint is working correctly.</li> </ol>"},{"location":"ckan_api_testing_prompts/#adding-comments-to-the-functions","title":"Adding Comments to the Functions","text":"<p>Prompt:</p> <p>Add comments to the below functions to explain what each function does, what parameters it takes, and what it returns.  Make sure to include a brief description of the purpose of the function and any important details that the user should be aware of when using it.</p> <p>Functions (Not Shown Here)</p> <p>Response:</p> <p>(Not Shown Here)</p> <p>Can be found in the CKAN API Testing Scripts.</p>"},{"location":"ckan_api_testing_scripts/","title":"CKAN API Testing Scripts","text":"<p>Note: This code is from Day 7 (6/11/24) of the logbook.</p>"},{"location":"ckan_api_testing_scripts/#initial-setup","title":"Initial Setup","text":"<pre><code># Import the necessary libraries\nimport requests\nimport json\n\n# Set the URL for the CKAN API\n# This is the main endpoint for accessing the CKAN API, which provides services like dataset creation,\n# resource management, and more.\nckan_url = 'https://ckan.cyverse.rocks/'\n\n# Set the API key for the CKAN API\n# The API key is used for authentication when making requests to the CKAN API.\n# Replace 'API_KEY' with your actual API key to access the API services.\napi_key = 'API_KEY'\n</code></pre>"},{"location":"ckan_api_testing_scripts/#create-a-dataset","title":"Create a Dataset","text":"<pre><code>def create_dataset(data):\n    \"\"\"\n    Create a new dataset in CKAN.\n\n    This function sends a POST request to the CKAN API to create a new dataset with the provided metadata.\n    The dataset metadata should include information such as name, title, description, owner organization,\n    and any additional metadata fields.\n\n    Args:\n        data (dict): The dataset metadata dictionary, including keys like 'name', 'title', 'description',\n                     'owner_org', and any additional metadata.\n\n    Returns:\n        dict: The response from the CKAN API, typically containing the dataset metadata.\n    \"\"\"\n    url = f'{ckan_url}/api/3/action/package_create'  # API endpoint for creating a dataset\n    headers = {\n        'Authorization': api_key,  # API key for authorization\n        'Content-Type': 'application/json'  # Content type for the request\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(data))  # Send POST request to create dataset\n    return response.json()  # Return the JSON response from the API\n\n\n# Example usage\ndataset_metadata = {\n    'name': 'my_dataset',\n    'title': 'My Dataset',\n    'notes': 'This is a test dataset created using the CKAN API.',\n    'owner_org': 'my_organization',\n    'tags': [{'name': 'example'}, {'name': 'dataset'}],\n    'author': 'John Doe',\n    'author_email': '\n}\nresponse = create_dataset(dataset_metadata)\n</code></pre>"},{"location":"ckan_api_testing_scripts/#delete-a-dataset","title":"Delete a Dataset","text":"<pre><code>def delete_dataset(dataset_id):\n    \"\"\"\n    Delete a dataset in CKAN.\n\n    This function deletes a specified dataset by sending a POST request to the CKAN API.\n    This is used in the migration script when a dataset needs to be updated or replaced.\n\n    Args:\n        dataset_id (str): The ID of the dataset to delete.\n\n    Returns:\n        dict: The response from the CKAN API.\n    \"\"\"\n    url = f'{ckan_url}/api/3/action/package_delete'  # API endpoint for deleting a dataset\n    headers = {\n        'Authorization': api_key,  # API key for authorization\n        'Content-Type': 'application/json'  # Content type for the request\n    }\n    data = {\n        'id': dataset_id  # ID of the dataset to delete\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(data))  # Send POST request to delete dataset\n    return response.json()  # Return the JSON response from the API\n\n\n# Example usage\ndataset_name = 'My Dataset'\ndataset_id = get_dataset_id(dataset_name)\ndelete_response = delete_dataset(dataset_id)\nprint(f'Delete response: {delete_response}')\n</code></pre>"},{"location":"ckan_api_testing_scripts/#delete-all-datasets-in-an-organization","title":"Delete all Datasets in an Organization","text":"<pre><code>def delete_all_datasets_in_organization(organization):\n    \"\"\"\n    Delete all datasets in a specific organization in CKAN.\n\n    This function deletes all datasets belonging to a specified organization by first listing all datasets\n    in the organization and then sending a delete request for each dataset.\n    This can be used to clean up existing datasets before migrating new data.\n\n    Args:\n        organization (str): The name or ID of the organization.\n\n    Returns:\n        None\n    \"\"\"\n    datasets = list_datasets(organization=organization)  # List all datasets in the organization\n    for dataset in datasets:\n        dataset_id = dataset['result']['id']  # Get the dataset ID\n        delete_response = delete_dataset(dataset_id)  # Delete the dataset\n        print(f'Deleted dataset with ID: {dataset_id}. Response: {delete_response}')  # Print confirmation message\n\n# Example usage\norganization_name = 'My Organization'\ndelete_all_datasets_in_organization(organization_name)\n</code></pre>"},{"location":"ckan_api_testing_scripts/#get-the-id-of-a-dataset","title":"Get the ID of a Dataset","text":"<pre><code>def get_dataset_id(dataset_name):\n    \"\"\"\n    Get the dataset ID for a given dataset name in CKAN.\n\n    This function retrieves the ID of a dataset by its name. It sends a GET request to the CKAN API\n    to fetch the dataset metadata and extract the dataset ID.\n\n    Args:\n        dataset_name (str): The name of the dataset.\n\n    Returns:\n        str: The dataset ID if found, or None if the dataset does not exist.\n    \"\"\"\n    dataset_name = dataset_name.lower().replace(' ', '-').replace('.', '-')  # Format dataset name to match CKAN conventions\n    url = f'{ckan_url}/api/3/action/package_show'  # API endpoint for showing dataset details\n    headers = {'Authorization': api_key}  # API key for authorization\n    params = {'id': dataset_name}  # Parameters for the GET request\n    response = requests.get(url, headers=headers, params=params)  # Send GET request to retrieve dataset details\n    if response.status_code == 200:\n        dataset_metadata = response.json()  # Parse the JSON response\n        if dataset_metadata['success']:\n            return dataset_metadata['result']['id']  # Return dataset ID if found\n        else:\n            print(f\"Error: {dataset_metadata['error']['message']}\")  # Print error message if not successful\n            return None\n    elif response.status_code == 404:\n        print(f\"Dataset '{dataset_name}' not found.\")  # Print message if dataset not found\n        return None\n    else:\n        print(f\"An error occurred: {response.status_code} - {response.text}\")  # Print error message for other errors\n        return None\n\n# Example usage\ndataset_name = 'My Dataset'\ndataset_id = get_dataset_id(dataset_name)\n</code></pre>"},{"location":"ckan_api_testing_scripts/#upload-a-resource-to-a-dataset","title":"Upload a Resource to a Dataset","text":"<pre><code>def upload_resource(dataset_id, file_path, name, date_created, date_updated, description=None):\n    \"\"\"\n    Upload a resource (file) to a CKAN dataset.\n\n    This function uploads a file to a specified CKAN dataset by sending a POST request to the CKAN API.\n    The function attaches the file and metadata such as the resource name and description.\n\n    Args:\n        dataset_id (str): The ID of the dataset to add the resource to.\n        file_path (str): The local path to the file to upload.\n        name (str): The name of the resource.\n        description (str, optional): A brief description of the resource.\n        date_created (str): The date the resource was created.\n        date_updated (str): The date the resource was last updated.\n\n    Returns:\n        dict: The response from the CKAN API, typically containing the resource metadata.\n    \"\"\"\n    url = f'{ckan_url}/api/3/action/resource_create'  # API endpoint for creating a resource\n    headers = {\n        'Authorization': api_key  # API key for authorization\n    }\n    data = {\n        'package_id': dataset_id,  # ID of the dataset to add the resource to\n        'name': name,  # Name of the resource\n        'description': description,  # Description of the resource\n        'date_created_de': date_created,  # Date the resource was created\n        'date_updated_de': date_updated  # Date the resource was last updated\n    }\n    files = {\n        'upload': open(file_path, 'rb')  # File to upload\n    }\n    response = requests.post(url, headers=headers, data=data, files=files)  # Send POST request to upload resource\n    return response.json()  # Return the JSON response from the API\n\n# Example usage\ndataset_name = 'My Dataset'\ndataset_id = get_dataset_id(dataset_name)\nresource_response = upload_resource(dataset_id, file_path='example.csv', resource_name='Example Resource')\n</code></pre>"},{"location":"ckan_api_testing_scripts/#add-a-resource-using-a-link","title":"Add a resource using a link","text":"<pre><code>def add_resource_link(data):\n    \"\"\"\n    Add a link to a resource in a CKAN dataset.\n\n    This function adds a URL link to a specified CKAN dataset by sending a POST request to the CKAN API.\n    This is the primary way that resources are added to CKAN datasets from the discovery environment.\n    The resource metadata should include the dataset ID, URL, name, description, format, and relevant dates.\n\n    Args:\n        data (dict): The resource metadata dictionary, including keys like 'package_id', 'name', 'url',\n                     'description', 'format', and relevant dates.\n\n    Returns:\n        dict: The response from the CKAN API, typically containing the resource metadata.\n    \"\"\"\n    resource_url = f'{ckan_url}/api/3/action/resource_create'  # API endpoint for creating a resource\n    headers = {\n        'Authorization': api_key  # API key for authorization\n    }\n    response = requests.post(resource_url, headers=headers, json=data)  # Send POST request to add resource link\n    return response.json()  # Return the JSON response from the API\n\n# Example usage\nresource_data = {\n    'package_id': 'my_dataset_id',\n    'name': 'Example Resource',\n    'url': 'https://example.com/data.csv',\n    'description': 'This is an example resource link.',\n    'format': 'CSV',\n    'Date Created in Discovery Environment': '2024-06-11',\n    'Date Last Modified in Discovery Environment': '2024-06-11'\n}\nresponse = add_resource_link(resource_data)\n</code></pre>"},{"location":"ckan_api_testing_scripts/#get-dataset-information","title":"Get dataset information","text":"<pre><code>def get_dataset_info(dataset_id):\n    \"\"\"\n    Get detailed information about a specific dataset in CKAN.\n\n    This function retrieves detailed metadata for a specified dataset by sending a GET request to the CKAN API.\n    It is used to fetch information such as the dataset title, description, tags, resources, and other metadata.\n\n    Args:\n        dataset_id (str): The ID of the dataset to retrieve information about.\n\n    Returns:\n        dict: The response from the CKAN API, containing the dataset metadata.\n    \"\"\"\n    url = f'{ckan_url}/api/3/action/package_show'  # API endpoint for showing dataset details\n    headers = {'Authorization': api_key}  # API key for authorization\n    params = {'id': dataset_id}  # Parameters for the GET request\n    response = requests.get(url, headers=headers, params=params)  # Send GET request to retrieve dataset details\n    return response.json()  # Return the JSON response from the API\n\n\n# Example usage\ndataset_name = 'My Dataset'\ndataset_id = get_dataset_id(dataset_name)\ndataset_info = get_dataset_info(dataset_id)\nprint(f'Dataset information: {dataset_info}')\n</code></pre>"},{"location":"ckan_api_testing_scripts/#list-all-datasets-in-the-ckan-instance","title":"List all Datasets in the CKAN instance","text":"<pre><code>def list_datasets(organization=None, group=None):\n    \"\"\"\n    List all datasets in the CKAN instance for a specific organization and/or group.\n\n    This function retrieves a list of all datasets in the CKAN instance, filtered by organization\n    or group if specified. It sends a GET request to the CKAN API and returns the dataset metadata.\n    This is used in the migration process to check whether a dataset from the discovery environment already exists in CKAN.\n\n    Args:\n        organization (str, optional): The name or ID of the organization.\n        group (str, optional): The name or ID of the group.\n\n    Returns:\n        dict: The response from the CKAN API, containing a list of dataset IDs.\n    \"\"\"\n    url = f'{ckan_url}/api/3/action/package_list'  # API endpoint for listing datasets\n    headers = {'Authorization': api_key}  # API key for authorization\n    output = []\n\n    if organization is not None:\n        # If an organization is specified, get the list of datasets for that organization\n        url = f'{ckan_url}/api/3/action/organization_show'\n        response = requests.get(url, headers=headers, params={'id': organization, 'include_datasets': True})\n        response = response.json()\n        response = response['result']['packages']\n        for dataset in response:\n            output.append(get_dataset_info(dataset['id']))\n        return output\n\n    elif group is not None:\n        # If a group is specified, get the list of datasets for that group\n        url = f'{ckan_url}/api/3/action/group_show'\n        response = requests.get(url, headers=headers, params={'id': group, 'include_datasets': True})\n        response = response.json()\n        response = response['result']['packages']\n        for dataset in response:\n            output.append(get_dataset_info(dataset['id']))\n        return output\n\n    else:\n        # If neither an organization nor a group is specified, get the list of all datasets\n        response = requests.get(url, headers=headers)\n        return response.json()\n\n\n# Example usage\ndatasets = list_datasets()\nprint(f'List of datasets: {datasets}')\n\n# Example usage with organization\norganization_name = 'My Organization'\norganization_datasets = list_datasets(organization=organization_name)\n\n# Example usage with group\ngroup_name = 'My Group'\ngroup_datasets = list_datasets(group=group_name)\n</code></pre>"},{"location":"ckan_api_testing_scripts/#print-json-data-in-a-readable-format","title":"Print json data in a readable format","text":"<pre><code>def pretty_print(json_data):\n    \"\"\"\n    Format and print JSON data in a readable way.\n\n    This function formats JSON data with indentation and sorted keys to make it more readable\n    when printed to the console.\n\n    Args:\n        json_data (dict): JSON data to be pretty-printed.\n    \"\"\"\n    print(json.dumps(json_data, indent=4, sort_keys=True))\n\n# Example usage\npretty_print(dataset_info)\n</code></pre>"},{"location":"croissant_notes/","title":"Croissant Notes","text":"<p>Note: These notes are from Day 16 (6/25/24) and Day 17 (6/26/24) of the logbook.</p>"},{"location":"croissant_notes/#croissant-overview","title":"Croissant Overview","text":"<ul> <li>Croissant is a new metadata format for standardizing machine learning datasets.</li> <li>The puropse of Croissant is to make datasets easily discoverable and useable across many different tools and platforms.</li> <li>Features<ul> <li>Adds metadata to describe datasets content in a standardized way.</li> <li>Enables loading datasets into different ML platforms without reformatting.</li> <li>Supported by major repositories like Kaggle, HuggingFace, and OpenML as well as Google Dataset Search</li> </ul> </li> <li>Benefits<ul> <li>Simplifies dataset discovery and usage.</li> <li>Reduces the need for manual data preprocessing.</li> <li>Facilitates collaboration and reproducibility in ML research.</li> </ul> </li> <li>Croissant Layers<ul> <li>Dataset Metadata Layer: General info (name, description, license)</li> <li>Resources Layer: Describes files (FileObject) and sets of files (FileSet)</li> <li>Structure Layer: Describes and organizes structure of resources (RecordSets)</li> <li>Semantic Layer: adAdds ML-specific interpretations (e.g., data types, train/test splits)</li> </ul> </li> <li>Future Directions<ul> <li>Community involvement to enhance ML specific aspects</li> <li>Adoption beyond ML for broader data interoperability</li> </ul> </li> </ul>"},{"location":"de_api_testing_scripts/","title":"Discovery Environment API Testing Scripts","text":"<p>Note: This code is from Day 9 (6/13/24) of the logbook.</p>"},{"location":"de_api_testing_scripts/#initial-setup","title":"Initial Setup","text":"<pre><code>import requests\nimport json\nfrom datetime import datetime\n\n# Base URL for the Discovery Environment API\n# This is the main endpoint for accessing the Discovery Environment API, which provides various services\n# like metadata retrieval, file management, and more.\nbase_url = 'https://de.cyverse.org/terrain'\n\n# API Key (replace with your actual API key)\n# The API key is used for authentication when making requests to the Discovery Environment API.\n# Replace 'YOUR_API_KEY' with your actual API key to access the API services.\napi_key = 'Bearer \" + 'YOUR_API_KEY'\n\n# Headers for the requests\n# These headers include the API key and are required for making authorized requests to the API.\nheaders = {\n    'Authorization': api_key\n}\n\n---\n\n## Print JSON Data in a Readable Format\n\n```python\ndef pretty_print(json_data):\n    \"\"\"\n    Format and print JSON data in a readable way.\n\n    This function takes JSON data and prints it in a formatted, easy-to-read manner.\n    Useful for debugging and ensuring the correct data structure is being used.\n\n    Args:\n        json_data (dict): JSON data to be pretty-printed.\n    \"\"\"\n    print(json.dumps(json_data, indent=4, sort_keys=True))\n</code></pre>"},{"location":"de_api_testing_scripts/#convert-milliseconds-since-epoch-to-date","title":"Convert Milliseconds since Epoch to Date","text":"<pre><code>def convert_to_date(milliseconds):\n    \"\"\"\n    Convert milliseconds since epoch to a human-readable date and time.\n\n    This function converts a timestamp in milliseconds to a human-readable date and time string.\n    This is used to convert the milliseconds since epoch of the date created and date updated fields to a more readable format\n\n    Args:\n        milliseconds (int): Milliseconds since epoch.\n\n    Returns:\n        str: Human-readable date and time.\n    \"\"\"\n    seconds = milliseconds / 1000  # Convert milliseconds to seconds\n    date_obj = datetime.fromtimestamp(seconds)  # Create a datetime object from the timestamp\n    date_str = date_obj.strftime('%Y-%m-%d %H:%M:%S')  # Format the datetime object as a string\n    return date_str\n</code></pre>"},{"location":"de_api_testing_scripts/#get-the-metadata-for-a-specific-data-id","title":"Get the metadata for a specific data id","text":"<pre><code>def get_metadata(data_id):\n    \"\"\"\n    Get metadata for a specific data ID.\n\n    This function retrieves metadata for a specified dataset by its ID.\n    It sends a GET request to the Discovery Environment API.\n\n    Args:\n        data_id (str): The ID of the data item.\n\n    Returns:\n        dict: The metadata for the specified data ID.\n    \"\"\"\n    url = f'{base_url}/filesystem/{data_id}/metadata'  # Construct the API URL for the metadata endpoint\n    response = requests.get(url, headers=headers)  # Send a GET request to the API\n    if response.status_code == 200:\n        metadata = response.json()  # Parse the JSON response\n        return metadata\n    else:\n        print(f\"Error getting metadata: {response.status_code} - {response.text}\")  # Print error message if the request fails\n        return None\n</code></pre>"},{"location":"de_api_testing_scripts/#get-all-metadata-for-a-dataset","title":"Get all metadata for a dataset","text":"<pre><code>def get_all_metadata_dataset(dataset):\n    \"\"\"\n    Get all metadata for a dataset.\n\n    This function collects all metadata for a given dataset, including creation and modification dates,\n    and detailed attributes from the Discovery Environment API. Used to migrate the datasets and their metadata\n    to CKAN.\n\n    Args:\n        dataset (dict): The dataset dictionary.\n\n    Returns:\n        dict: A dictionary containing all metadata for the dataset.\n    \"\"\"\n    metadata_dict = {}\n\n    # Convert and store creation and modification dates\n    date_created = convert_to_date(int(dataset['date-created']))  # Convert creation date to readable format\n    metadata_dict['date_created'] = date_created\n\n    date_modified = convert_to_date(int(dataset['date-modified']))  # Convert modification date to readable format\n    metadata_dict['date_modified'] = date_modified\n\n    metadata_dict['de_path'] = dataset['path']  # Store the dataset path\n\n    dataset_id = dataset['id']  # Get the dataset ID\n\n    # Get detailed metadata from the API\n    metadata_return = get_metadata(dataset_id)\n    avus = metadata_return['avus']  # Get attribute-value units (AVUs)\n\n    # Loop through each AVU and add it to the metadata dictionary\n    for avu in avus:\n        key = avu['attr']\n        value = avu['value']\n        if key in metadata_dict:\n            try:\n                metadata_dict[key].append(value)\n            except AttributeError:\n                metadata_dict[key] = [metadata_dict[key], value]\n        else:\n            metadata_dict[key] = value\n\n    return metadata_dict\n</code></pre>"},{"location":"de_api_testing_scripts/#get-all-metadata-for-a-file","title":"Get all metadata for a file","text":"<pre><code>def get_all_metadata_file(file):\n    \"\"\"\n    Get metadata for a specific file.\n\n    This function collects all metadata for a given file, including creation and modification dates,\n    file type, and WebDAV location. Used for migrating files to CKAN.\n\n    Args:\n        file (dict): The file dictionary.\n\n    Returns:\n        dict: A dictionary containing all metadata for the file.\n    \"\"\"\n    metadata_dict = {}\n\n    # Convert and store creation and modification dates\n    date_created = convert_to_date(int(file['date-created']))  # Convert creation date to readable format\n    metadata_dict['date_created'] = date_created\n\n    date_modified = convert_to_date(int(file['date-modified']))  # Convert modification date to readable format\n    metadata_dict['date_modified'] = date_modified\n\n    metadata_dict['de_path'] = file['path']  # Store the file path\n\n    file_name = file['label']  # Get the file name\n    metadata_dict['file_name'] = file_name\n\n    # Get the file type from the label\n    file_type = file_name.split('.')[-1]\n    if file_type == file_name:\n        file_type = ''\n    metadata_dict['file_type'] = file_type\n\n    # Construct the WebDAV location URL\n    web_dav_location = \"https://data.cyverse.org/dav-anon/iplant/commons/cyverse_curated/\" + file['path'].replace('/iplant/home/shared/commons_repo/curated/', '')\n    metadata_dict['web_dav_location'] = web_dav_location\n\n    return metadata_dict\n</code></pre>"},{"location":"de_api_testing_scripts/#get-the-list-of-filesfolders-in-a-specified-directory","title":"Get the list of files/folders in a specified directory","text":"<pre><code>def get_files(path, limit=10):\n    \"\"\"\n    Get the list of files and folders in a specified directory.\n\n    This function retrieves a list of files and folders in a specified directory from the Discovery Environment API.\n    Useful for migrating files/foldersfrom a directory to CKAN.\n\n    Args:\n        path (str): The path to the directory.\n        limit (int): The maximum number of files to retrieve.\n\n    Returns:\n        dict: A dictionary containing the list of files.\n    \"\"\"\n    url = f'{base_url}/secured/filesystem/paged-directory'  # Construct the API URL for the directory endpoint\n    params = {'limit': limit, 'path': path}  # Set the request parameters\n    response = requests.get(url, headers=headers, params=params)  # Send a GET request to the API\n    if response.status_code == 200:\n        files = response.json()  # Parse the JSON response\n        return files\n    else:\n        print(f\"Error getting files: {response.status_code} - {response.text}\")  # Print error message if the request fails\n        return None\n</code></pre>"},{"location":"de_api_testing_scripts/#get-a-list-of-all-datasets-with-some-of-their-metadata","title":"Get a list of all datasets with some of their metadata","text":"<pre><code>def get_datasets():\n    \"\"\"\n    Get a list of all datasets with some of their metadata. The rest of the metadata can be retrieved using the get_metadata function.\n\n    This function retrieves a list of all datasets in a specified path from the Discovery Environment API.\n    Used in conjunction with get_all_metadata_dataset function to migrate datasets and their metadata\n    to CKAN.\n\n    Args:\n        None\n\n    Returns:\n        list: A list of dictionaries, each representing a dataset with its metadata.\n    \"\"\"\n    path = '/iplant/home/shared/commons_repo/curated/'  # Define the path to the datasets\n    url = f'{base_url}/secured/filesystem/directory'  # Construct the API URL for the directory endpoint\n    params = {'path': path}  # Set the request parameters\n    response = requests.get(url, headers=headers, params=params)  # Send a GET request to the API\n    if response.status_code == 200:\n        directories = response.json()  # Parse the JSON response\n        datasets = directories['folders']  # Extract the list of datasets\n        return datasets\n    else:\n        print(f\"Error getting directories: {response.status_code} - {response.text}\")  # Print error message if the request fails\n        return None\n</code></pre>"},{"location":"discovery_environment_api_prompts/","title":"Discovery Environment API Prompts","text":"<p>Note: These prompts are from Day 8 (6/12/24) of the logbook.</p> <p>System Instructions:</p> <p>Same as the instructions for Example 1 in practice.</p>"},{"location":"discovery_environment_api_prompts/#finding-api-endpoints-to-access-curated-datasets","title":"Finding API Endpoints to Access Curated Datasets","text":"<p>Prompt:</p> <p>Now I need to access the API for the current discovery environment which has the data commons right now.  Again, the goal is to write a script to transfer the datasets from this current discovery environment to the new CKAN environment.  I am going to give you a list of endpoints for the current discovery environment and I need you to tell me which of the possible endpoints will let me access the datasets in the discovery environment.</p> <p>(List of endpoints not shown here but can be found Discovery Environment API Documentation)</p> <p>Response:</p> <p>To identify which endpoints you can use to access the datasets in the discovery environment, you should look for endpoints related to data, files, and file systems. Based on the provided list, the relevant endpoints are under the \"data\" and \"filesystem\" categories. Here are the potential endpoints that you can use:</p> <p>Data Endpoints</p> <ul> <li>GET /terrain/secured/saved-searches: Get Saved Searches</li> <li>GET /terrain/saved-searches: Get Saved Searches</li> </ul> <p>Filesystem Endpoints</p> <ul> <li>GET /terrain/secured/filesystem/root: Root Listing</li> <li>GET /terrain/secured/filesystem/directory: Directory List (Non-Recursive)</li> <li>GET /terrain/secured/filesystem/display-download: Display and Download File</li> <li>GET /terrain/secured/filesystem/paged-directory: List Folder Contents</li> <li>GET /terrain/secured/filesystem/file/manifest: Get File Manifest</li> <li>GET /terrain/secured/filesystem/search-documentation: Get additional documentation for the search endpoint</li> <li>GET /terrain/secured/filesystem/entry/{entry-id}/comments: Get File or Folder Comments</li> </ul> <p>File I/O Endpoints</p> <ul> <li>GET /terrain/secured/fileio/download: Retrieve File Contents</li> </ul>"},{"location":"discovery_environment_api_prompts/#accessing-metadata-for-specific-datasets","title":"Accessing Metadata for Specific Datasets","text":"<p>Prompt:</p> <p>I need to access the metadata for specific datasets in the discovery environment but I'm not sure which endpoint to use. Can you tell me which endpoint I should use to get the metadata for a specific dataset like the DOI, creator, description, etc?</p> <p>Response:</p> <p>To get the metadata of a dataset, including information such as the DOI, creator, description, and other relevant metadata, you should look for an endpoint that provides detailed information about the dataset or a specific file's metadata. Based on the provided endpoints, the following could be used:</p> <p>Relevant Endpoints</p> <ul> <li>GET /terrain/secured/filesystem/{data-id}/metadata: This endpoint retrieves the metadata of a specific data item (file or folder) by its ID. This should include various attributes such as the creator, description, and potentially the DOI if it is stored as part of the metadata.</li> </ul>"},{"location":"github/","title":"Github Actions","text":""},{"location":"github/#introduction","title":"Introduction","text":"<p>GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. </p>"},{"location":"github/#key-vocabulary","title":"Key Vocabulary","text":""},{"location":"github/#workflows","title":"Workflows","text":"<p>Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. </p>"},{"location":"github/#events","title":"Events","text":"<p>Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. </p>"},{"location":"github/#runners","title":"Runners","text":"<p>Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. </p>"},{"location":"github/#jobs","title":"Jobs","text":"<p>A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. </p>"},{"location":"github/#steps","title":"Steps","text":"<p>A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.</p>"},{"location":"github/#actions","title":"Actions","text":"<p>Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. </p>"},{"location":"github/#github-education","title":"GitHub Education","text":""},{"location":"github/#steps-to-enroll","title":"Steps to Enroll","text":"<ol> <li>Go to the GitHub Education Site and enter your education status as student</li> <li>From here your school email and dated documentation of your enrollment is required</li> <li>After this is approved you have access to the GitHub education student developer pack! </li> </ol>"},{"location":"github/#whats-included-and-functionality","title":"What's Included and Functionality","text":"<p>As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. </p> <p>note: the student developer pack doesn't include access to GitHub CodeSpaces </p>"},{"location":"keysassignments/","title":"KEYS Assignments","text":"<p>Internship Description</p> <p>Introduction</p> <p>Materials and Methods</p> <p>Results</p> <p>Long Abstract</p> <p>Poster Title and Short Abstract</p> <p>Poster</p>"},{"location":"logbook/","title":"Logbook","text":""},{"location":"logbook/#training-week","title":"Training Week","text":""},{"location":"logbook/#day-1-6324","title":"Day 1 (6/3/24)","text":"<p>Started off the day with learning about science literacy, research posters, and scientific wriitng. Then, I learned the basics of command line and scripting with Dr. Hilgert.</p>"},{"location":"logbook/#day-2-6424","title":"Day 2 (6/4/24)","text":"<p>Learned presentation skills and then how to create a poster in PowerPoint. After that, I learned about remote computing and then connected to the University of Arizona's HPC.</p>"},{"location":"logbook/#day-3-6524","title":"Day 3 (6/5/24)","text":"<p>Learned the importance of responsible science and research communication and learned about machine learning from Dr. Greg Chism.</p> <p>I also met with Dr. Tyson Swetnam and Dr. Tony Edgin to discuss the details of the internship. They introduced me to the CyVerse Data Commons, CKAN, and iRODS which are the main tools that I will be using for the internship. They explained that the goal of the project is to allow a researcher to upload their data onto the CyVerse Data Commons and then use the CKAN API to access the data. We also discussed the importance of prompt engineering in order to be able to generate helpful code. </p>"},{"location":"logbook/#day-4-6624","title":"Day 4 (6/6/24)","text":"<p>Learned how to use excel and visualize data with the help of Mollie Wiegand. Learned about statistics in a lecture given by Dr. Greg Chism. Then, I learned the basis of using R and RStudio as well as image analysis using ImageJ.</p>"},{"location":"logbook/#day-5-6724","title":"Day 5 (6/7/24)","text":"<p>Discussed a research paper on science literacy with other interns. Attended a lecture on bioinformatics given by Dr. Uwe Hilgert. Attended a lecture on professionalism given by Kelle Hyland and completed the training week debrief and evaluation.</p>"},{"location":"logbook/#keys-internship","title":"KEYS Internship","text":""},{"location":"logbook/#day-6-61024","title":"Day 6 (6/10/24)","text":"<p>Today, I set up my OpenAI account so that I could practice prompt engineering. I watched some YouTube videos on prompt engineering and read through the website provided by Dr. Swetnam.  Here are some examples that I created.</p> <p>I also explored the CKAN deployment for CyVerse within an example organization. I learned how to create a dataset and how to add metadata to it. Additionally, I worked on my project description and the layout of my poster.</p>"},{"location":"logbook/#day-7-61124","title":"Day 7 (6/11/24)","text":"<p>I manually transferred existing datasets from the curated data commons to the CKAN deployment to get a feel of how the datasets are structured. </p> <p>I also learned how to use the CKAN API to automate the process of transferring datasets. I created several different functions in Python that interact with the CKAN API to create/delete datasets, add resources, create organizations, and get dataset information.  These functions were created with the help of ChatGPT Prompt Engineering and using the CKAN API documentation. These functions can be found here. One obstacle that I encountered was that I didn't exactly know how to create some functions like the get_dataset_id function. To solve this, I used prompt engineering to ask ChatGPT to help me create the function.  Another obstacle was that the get_dataset_id function that was provided to me by ChatGPT wasn't working at first and it was saying that the dataset was not found.  After asking ChatGPT for help, I realized that the dataset name has to be in all lowercase and has to have hyphens instead of spaces.</p> <p>In the next few days, I plan to get familiar with the API for the existing data commons so that in the future I can automate the process of transferring datasets from the existing data commons to the CKAN deployment.</p>"},{"location":"logbook/#day-8-61224","title":"Day 8 (6/12/24)","text":"<p>In the morning, I attended the weekly science seminar and learned more about science literacy and attended lectures by Dr. Josh Nagao and Dr. Aisha Ahmed.</p> <p>Afterwords, Tony Edgin introduced me to the Discovery Environment API that I could use to access the curated datasets in the datacommons.  I was told to find the API endpoints that would allow me to access the datasets as well as the metadata for those datasets. Because of the vast quantity of API endpoints, I asked ChatGPT to help me find the endpoints that I needed. The prompts that I used can be found here.</p> <p>Below are some of the endpoints that will be important for me to use:</p> <ul> <li>GET /terrain/secured/filesystem/directory: Get the list of directories in a specified path </li> <li>GET /terrain/secured/filesystem/paged-directory: Get the list of files in a specified directory </li> <li>GET /terrain/secured/filesystem/{data-id}/metadata: Get the metadata for a specific directory using its id</li> </ul>"},{"location":"logbook/#day-9-61324","title":"Day 9 (6/13/24)","text":"<p>I started creating python functions that interact with the Discovery Environment API.  I used the endpoints that I found yesterday to create these functions. During the standup meeting, I was told to determine if any of the datasets have ARK permanent identifiers so using the functions that I created, I was able to find that there were 17 of the datasets that had ARK permanent identifiers. The process of migrating these datasets to the CKAN deployment will be slightly different so it is important to know which datasets have ARK permanent identifiers. Lastly, I was working on getting the metadata for the datasets and files so that I could store it in an organized manner. This will make it easier when I am creating the datasets in the CKAN deployment.</p> <p>The Python code that I created can be found here.</p>"},{"location":"logbook/#day-10-61424","title":"Day 10 (6/14/24)","text":"<p>To start off the day, I revised the functions that I created in the past few days and mentally mapped out the process of transferring datasets from the existing data commons to the CKAN deployment. When I was doing this, I realized that there was no way to edit the modified date and the created date of CKAN folders so upon asking my lab mentors, it was determined that I would not map the modified date and the created date of the datasets to the CKAN deployment. Furthermore, I also realized that there wasn't a field for the modified and created dates for the resources in the CKAN deployment.  However, I realized that I could add custom fields to the resources which is what I will be doing.</p> <p>I then started creating the script that will automate the process of transferring datasets from the existing data commons to the CKAN deployment. I was first having difficulty figuring out how to map the license metadata because the exact name of the license is different even though they refer to the same license. To solve this, I had to create sample datasets in the CKAN deployment and then use the functions I created to see the metadata that was returned.  The metadata showed that I had to create three new fields depending on the type of license so that it would properly show up in the CKAN deployment.</p> <p>Another issue I had was that the subject metadata in the existing data commons was either a single string or a list of strings there were errors when I was trying to map them to the tags in the CKAN deployment. Therefore, I had to first check if the subject metadata was a list and then iterate through the list to add each subject as a tag.</p> <p>Another issue I was encountering was that some of the metadata keys were under different names but referred to the same thing such as the title, author name, etc. Some of the datasets had \"datacite.\" in front of the keys so I was working on adjusting the script to handle this.  However, I am not done with this and will need to finish it on Monday.</p> <p>In addition, I still need to start working on the part of the script that will map the resources of the datasets.</p>"},{"location":"logbook/#day-11-61724","title":"Day 11 (6/17/24)","text":"<p>I finished the migration script that will automate the process of transferring datasets from the existing data commons to the CKAN deployment that I was working on last Friday. Something that took a considerable amount of time was creating the citations for the datasets because that metadata wasn't available in the existing data commons. I had to manually create a function that would create the citation for the datasets based on the metadata. However, I realized that some of the fields for the citation like the creator, title, publication year, etc. were stored slightly differently in the existing data commons so I had to adjust the function to handle this.</p> <p>I was also having some trouble with deciding how I would make the script idempotent meaning that the script won't add anything to CKAN that is already present, but it will update information that has changed. Specifically, I wasn't sure how I would update any changes to the CKAN dataset that were made to the datasets in the existing data commons. I then realized that I could use the date modified field in the existing data commons to check if the dataset has been modified and then update the dataset in the CKAN deployment by deleting the dataset and then creating it again with the new metadata. This would make it easier to update the datasets in the CKAN deployment rather than having to manually check each metadata field and resource for changes.</p> <p>The migration script that I created can be found here. The entire code so far can also be found on GitHub here.</p>"},{"location":"logbook/#day-12-61824","title":"Day 12 (6/18/24)","text":"<p>I started running the script that I created to transfer the datasets from the existing data commons to the CKAN deployment. While running the script, there were several problems that came up:</p> <ol> <li>My function to get the details of all the datasets in CKAN was only returning the first 10 datasets.  I couldn't exactly figure out the problem by myself so I asked ChatGPT for help. I told it what the problem was and it gave me a solution that I could use to fix the problem. However, the solution that ChatGPT gave me didn't work so I had to ask ChatGPT for help again. This happened a few times until it finally gave me a solution that worked.</li> <li>There were two datasets that had the same title despite it having different metadata so my script was throwing an error.  I asked Tony Edgin and Michele Cosi for help, and they fixed the title in the existing data commons so that the script would run without any errors.</li> <li>Some of the titles were longer than 100 characters and the CKAN API wasn't letting me create a dataset whose name was longer than 100 characters. I asked Tony Edgin for help and he told me to just truncate the name to 100 characters. The title of the dataset is different from the name of the dataset so it was okay to truncate the name.</li> <li>There is a dataset with no metadata in the existing data commons. I asked my lab mentors for help but I am still waiting on a reply from Dr. Merchant.</li> <li>There are several datasets with many files and the script was taking a long time to run for these datasets. Therefore, I skipped these datasets for now and I will ask my lab mentors for help on how to handle these datasets.</li> </ol> <p>There were other minor errors that I encountered, mostly dealing with characters that weren't allowed in the name of the dataset or the tags. These were quite easy to fix and I just had to add a bit of code to replace those characters with something else.</p>"},{"location":"logbook/#day-13-62024","title":"Day 13 (6/20/24)","text":"<p>In the morning, I attended the weekly science seminar and learned more about science literacy and attended lectures by Dr. Chris Frost. After the seminar, I attended the standup meeting with Tony Edgin and Dr. Swetnam.  I told them about how it was taking a long time to run the script for datasets with many files and they showed me how to run the script in a Ubuntu VM that was hosted on CyVerse. This would allow me to easily run the script for datasets with many files because the VM had more resources than my local machine. This script is still running and I will check on it tomorrow. I also looked into flake8 which is a tool that checks for style errors in Python code, as suggested by Tony. Lastly, I looked into some documentation and resources for Gradio provided by Dr. Merchant.</p>"},{"location":"logbook/#day-14-62124","title":"Day 14 (6/21/24)","text":"<p>I checked on the script that was running in the Ubuntu VM and it is still running.  I finished the script that would take in command line arguments and run the migration script for a specific dataset. The arguments that the script would take in are the dataset name, the organization name, and the username and password for CyVerse in order to generate the API key.</p> <p>I also created a cron job on the CKAN host so that the script would run every day at 5:00 PM.</p> <p>Lastly, I finished the Materials and Methods section which can be found here.</p>"},{"location":"logbook/#day-15-62424","title":"Day 15 (6/24/24)","text":"<p>I checked on the script that was running in the Ubuntu VM again and there were errors that were thrown. I wasn't exactly sure what the error meant but when I reran the script I realized that the API key that I was using had expired. In order to solve this, I created a test account on CyVerse and created a function that would generate the API key using the username and password of the test account. Every time that the script runs, it will generate a new API key so that it hopefully doesn't expire while the script is running.</p> <p>Additionally, when I reran the script for the second time, I realized that something was wrong with the part of the script that was checking the last modified dates in the Discovery Environment API and CKAN because the CKAN dates were exactly 7 hours ahead of the Discovery Environment dates. However, when the datasets were originally created, the dates were the same so I am not exactly sure why the dates are different now. Until I figure out why the dates are different, I temporarily check if the last modified date in the Discovery Environment API is exactly 7 hours behind the last modified date in CKAN and if it is, I don't update the dataset in CKAN.</p> <p>Furthermore, I realized that some of the datasets hadn't transferred all of their files to the CKAN deployment so I wrote a function that would check if all of the files in the dataset were transferred and if they weren't, it would transfer the files that weren't transferred.</p> <p>I started working on figuring out how to disable the ability of regular users to create CKAN organizations but I couldn't figure out how to access the configuration file in the CKAN deployment to do this. I will ask Tony for help in the standup meeting tomorrow.</p> <p>While I was working on that, Tony got back to me about things that I should change in my migration script.  I spent the rest of the day working on that but I have still not finished it.</p>"},{"location":"logbook/#day-16-62524","title":"Day 16 (6/25/24)","text":"<p>At the beginning of the day, I continued working on the changes that Tony suggested for the migration script. In the middle of the day, I had a meeting with Dr. Merchant and Tony to discuss the progress of the project as well as future directions. We discussed the involvement of gradio in the project and how it could be used to create a user interface for the project which would ensure that the metadata that a user inputs is correct. We also discussed how something called croissant could be integrated with gradio.  Croissant is a metadata format for standardizing machine learning datasets. Apart from this, we talked about Amazon buckets which would allow to store vast amounts of data in an efficient way in CKAN. I was told to research on these topics and see how they could be integrated into the project. Today, I did research on croissant and my notes can be found here. I also created a sample gradio app that would work with the script to transfer a single dataset from the existing data commons to the CKAN deployment.</p>"},{"location":"logbook/#day-17-62624","title":"Day 17 (6/26/24)","text":"<p>In the morning we had the weekly science seminar where an alumni panel was held. We also learned about the KEYS slam which would be held the day before the research showcase.</p> <p>After the seminar, I found a research paper on croissant and read through it to get a better understanding of what croissant is. I added the notes from the research paper to my notes on croissant which can be found here.</p> <p>After this, I created an example workflow for how the gradio app would work with the script and croissant to transfer a dataset from the existing data commons to the CKAN deployment.</p> <p>Then, I researched BagIt which is a file packaging format that is used to store and transfer large amounts of data.  The notes from my research can be found here.</p>"},{"location":"logbook/#day-18-62724","title":"Day 18 (6/27/24)","text":"<p>For the majority of today, I was working on integrating the gradio app with the script that I already had.  I added validation to the app that would make sure the user's credentials, organization, and dataset name were correct. I did some more research on the exact structure of croissant metadata by looking at this google colab notebook that gave an example of how to create a croissant metadata file.</p> <p>I also met with Dr. Merchant, Tony, and Larry Head who is a professor at the University of Arizona working on the DRIVEArizona project so that I could use his datasets to test my project.</p>"},{"location":"logbook/#day-19-62824","title":"Day 19 (6/28/24)","text":"<p>Today, I was working on integrating croissant with the gradio interface.  I wanted the interface to be able to take in a DE path and then create a croissant metadata file for that path. If there was metadata that wasn't available in the DE path, the user would be able to input that metadata in the interface. However, I couldn't figure out how to add additional fields depending on what was missing. I asked ChatGPT for help but it wasn't able to give me a solution.</p>"},{"location":"logbook/#day-20-7124","title":"Day 20 (7/1/24)","text":"<p>Over the weekend, Dr. Merchant gave me a set of instructions to be working on which I will work on for the next few days. Today, I worked on the first step which was to create a parquet file using a file path in the Discovery Environment in a Jupyter notebook. While working on this, I encountered several errors and problems.  Firstly, I was having a hard time figuring out the API endpoint to get a file from the Discovery Environment. I asked ChatGPT for help and it was still not able to give me a solution. Eventually, I was able to figure out that I was using the wrong API endpoint and I was able to get the file from the Discovery Environment. Next, I was having trouble converting the csv file to a parquet file. The function that I was using was not storing the parquet file anywhere even though it was saying that it was. I asked ChatGPT for help and it provided a different function that stored the parquet file differently which worked.</p>"},{"location":"logbook/#day-21-7224","title":"Day 21 (7/2/24)","text":"<p>During the standup meeting today, I was told to try and create a VICE app that would have the croissant editor in it. I first ran the docker container from the croissant github repository locally which did work.  When I tried to create the VICE app, however, the app was not able to start. After asking Dr. Swetnam for help, he told me to hold off on the VICE app for now because we would need to create our own GitHub repository for it.</p> <p>Other than that, I worked on the Jupyter notebook from yesterday to add a functon which would combine all the csv files in a directory into a single parquet file by year.</p>"},{"location":"logbook/#day-22-7324","title":"Day 22 (7/3/24)","text":"<p>Today, I was working on creating a python script to be able to generate a DCAT JSON LD file from a some sample metadata. After I finished this, I was working on another python script which would be able take in a DCAT JSON LD file and create a Croissant metadata file from it. However, I kept encountering errors when trying to do this.  One of the main errors is that the properties \"md5\" or \"sha256\" must be defined in the json file.  I asked ChatGPT for help but it wasn't able to give me a solution. I will continue working on this tomorrow.</p>"},{"location":"logbook/#day-23-7524","title":"Day 23 (7/5/24)","text":"<p>I figured out the error that I was encountering yesterday with the \"md5\" and \"sha256\" properties. I was formatting the JSON file incorrectly so I fixed that and the error went away. I created another script to generate a Croissant metadata file from metadata. I started creating the gradio app from scratch because of all the errors I was getting previously.  I created 5 tabs in the app: </p> <ol> <li>one to transfer a dataset from the DE to CKAN</li> <li>one to create a Croissant metadata file from a DE link</li> <li>one to create a DCAT JSON LD file from a DE link</li> <li>one to create a dataset in CKAN from a Croissant metadata file</li> <li>one to create a dataset in CKAN from a DCAT JSON LD file.</li> </ol> <p>I plan to add more tabs such as creating a parquet file from a csv file and creating a parquet file from a directory of csv files. The issue that I was running into was that I wanted the app to be dynamic so that when the user enters a DE link and presses submit (for the first three tabs), there should be more fields that appear depending on what metadata is missing.</p>"},{"location":"logbook/#day-24-7824","title":"Day 24 (\u215e/24)","text":"<p>I asked Tony Edgin for help on the gradio app and he told me that I can hide and show fields depending on what the user inputs. I was able to implement this for my tabs.  I also implemented the functionality where the app gives an error if the user inputs a DE link that doesn't exist, if the username/password is incorrect, or if the input fields were left blank. Other than that, I simultaneously worked on the first three tabs of the app because they require very similar functions.</p>"},{"location":"logbook/#day-25-7924","title":"Day 25 (7/9/24)","text":"<p>CyVerse was down for most of the day so I wasn't able to work that much on the first three tabs because I wasn't able to test them. I mainly worked on the last two tabs of the app where I created functions to extract the metadata from the Croissant metadata file and the DCAT JSON LD file. For the 2<sup>nd</sup> and 3<sup>rd</sup> tabs, I was trying to figure out how the user could download the Croissant metadata file and the DCAT JSON LD file respectively. After looking at the documentation and some github posts, I was able to figure out how to do this. After CyVerse was back up, I worked on the first three tabs and I was able to complete the second one.  The third tab should be similar to the second one so I will be able to complete that tomorrow. I'm encountering an error for the first tab though and there is a problem with migrating the datasets to CKAN.</p>"},{"location":"logbook/#day-26-71024","title":"Day 26 (7/10/24)","text":"<p>Today was the weekly science seminar where the guests gave lectures about undergraduate research as well as the University of Arizona's Honors College. We also discussed the KEYS slam which would be held the day before the research showcase as well as the research showcase itself. I had to submit my poster to my KEYS crew mentor today so I mainly worked on that for the rest of the day.  For the error from yesterday regarding the first tab, I realized that the error was because there was no name key when uploading data to CKAN. However, my python script should be creating a name key so I'm not exactly sure why it isn't.</p>"},{"location":"logbook/#day-27-71124","title":"Day 27 (7/11/24)","text":"<p>Today, I mainly worked on my poster for the Research Showcase as well as the KEYS slam slides because I have to submit them tomorrow. Dr. Merchant, Dr. Swetnam, and Tony Edgin gave a lot of feedback for my rough draft which I incorporated into my final draft. I also met with Dr. Merchant and Tony Edgin to discuss what things to include in the poster and the slam slides.</p> <p>The poster can be found here.</p>"},{"location":"logbook/#day-28-71224","title":"Day 28 (7/12/24)","text":"<p>I finished basically all of my Gradio app today. I wasn't able to test it as much and I plan to do that on Monday and fix any errors that I encounter. I know that there could be some issues with DCAT JSON LD files and Parquet files especially because I didn't really get to test that code that much. I also did some finishing touches on my poster and slam slides and submitted them to Jordan Pilch.</p>"},{"location":"logbook/#day-29-71524","title":"Day 29 (7/15/24)","text":"<p>CyVerse and CKAN were down for most of the day because of a power outage so I wasn't able to test my Gradio app that much. I mainly worked on writing comments and restructuring my code so that it is easier to read and understand. I encountered some errors while I was doing this but I fixed those. I also pushed my code to the data commons GitHub repository. If CKAN is back up tomorrow, I will test my Gradio app and fix any errors that I encounter.</p> <p>I also added Devan Patel's (a fellow KEYS intern) chatbot to this website.</p>"},{"location":"migration_script/","title":"Migration Python Script","text":"<p>Note: This code is from Day 11 (6/17/24) of the logbook.</p> <pre><code>from CKAN_Testing import main as ckan\nfrom DE_API_Testing import main3 as de\nimport json\n\ndef clean_dataset_metadata(dataset_metadata: dict):\n    \"\"\"\n    Clean the dataset metadata by removing tabs from the values in the dictionary.\n    Args:\n        dataset_metadata (dict): The dataset metadata dictionary.\n\n    Returns:\n        dict: The cleaned dataset metadata dictionary.\n    \"\"\"\n    for key, value in dataset_metadata.items():\n        if isinstance(value, str):\n            dataset_metadata[key] = value.replace('\\t', '')\n        elif isinstance(value, list):\n            dataset_metadata[key] = [v.replace('\\t', '') for v in value]\n    return dataset_metadata\n\ndef create_citation(dataset_metadata: dict):\n    \"\"\"\n    Create a citation for the dataset using the dataset metadata.\n    Args:\n        dataset_metadata: The dataset metadata dictionary.\n\n    Returns:\n        str: The citation for the dataset.\n    \"\"\"\n\n    # Check if the dataset is a datacite dataset\n    is_datacite = 'datacite.creator' in dataset_metadata\n    citation = ''\n\n    # Add the author, publication year, title, and CyVerse Data Commons to the citation\n\n    citation += get_author(dataset_metadata) + \" \"\n\n    citation += get_publication_year(dataset_metadata) + \". \"\n\n    citation += get_title(dataset_metadata) + \". \"\n\n    citation += \"CyVerse Data Commons. \"\n\n    # Add the DOI identifier to the citation\n    try:\n        # Check if the dataset's identifier key is a string or a list\n        if isinstance(dataset_metadata['Identifier'], str):\n            # If the identifier is a string, add it to the citation\n            citation += f\"DOI {dataset_metadata['Identifier']}\"\n        # If the identifier is a list, check if the second element is an empty string. If it is, add only the first element to the citation. Otherwise, add both elements to the citation.\n        elif dataset_metadata['Identifier'][1] == '':\n            citation += f\"DOI {dataset_metadata['Identifier'][0]}\"\n        else:\n            citation += f\"DOI {dataset_metadata['Identifier'][0]}, {dataset_metadata['Identifier'][1]}\"\n    # If the dataset does not have an 'Identifier' key, check the 'identifier' key instead\n    except KeyError:\n        if isinstance(dataset_metadata['identifier'], str):\n            citation += f\"DOI {dataset_metadata['identifier']}\"\n        elif dataset_metadata['identifier'][1] == '':\n            citation += f\"DOI {dataset_metadata['identifier'][0]}\"\n        else:\n            citation += f\"DOI {dataset_metadata['identifier'][0]}, {dataset_metadata['identifier'][1]}\"\n\n    return citation\n\n\n\ndef get_title(dataset_metadata: dict):\n    \"\"\"\n    Get the title of the dataset from the dataset metadata.\n    Args:\n        dataset_metadata: The dataset metadata dictionary.\n\n    Returns:\n        str: The title of the dataset.\n    \"\"\"\n\n    # Check if the dataset is a datacite dataset\n    is_datacite = 'datacite.creator' in dataset_metadata\n\n    # Check if the dataset has a 'datacite.title' key. If it does, use that as the title. Otherwise, use the 'title' key.\n    if is_datacite:\n        try:\n            return dataset_metadata['datacite.title']\n        except KeyError:\n            return dataset_metadata['title']\n    ## If the dataset does not have a 'datacite.title' key, use the 'title' key or 'Title' key.\n    else:\n        try:\n            return dataset_metadata['title']\n        except KeyError:\n            return dataset_metadata['Title']\n\n\ndef get_author(dataset_metadata: dict):\n    \"\"\"\n    Get the author(s) of the dataset from the dataset metadata.\n    Args:\n        dataset_metadata: The dataset metadata dictionary.\n\n    Returns:\n        str: The author(s) of the dataset.\n    \"\"\"\n\n    # Check if the dataset is a datacite dataset\n    is_datacite = 'datacite.creator' in dataset_metadata\n\n    # Check if the dataset has a 'datacite.creator' key. If it does, use that as the author.\n    if is_datacite:\n        # If the author is a string, return the author. Otherwise, join the authors with a comma.\n        if isinstance(dataset_metadata['datacite.creator'], str):\n            return dataset_metadata['datacite.creator']\n        else:\n            return ', '.join(dataset_metadata['datacite.creator'])\n\n    # If the dataset does not have a 'datacite.creator' key...\n    else:\n        # If the author is a string, return the author. Otherwise, join the authors with a comma.\n        try:\n            if isinstance(dataset_metadata['creator'], str):\n                return dataset_metadata['creator']\n            else:\n                return ', '.join(dataset_metadata['creator'])\n        # If the dataset does not have a 'creator' key, return the 'Creator' key.\n        except KeyError:\n            return dataset_metadata['Creator']\n\n\ndef get_publication_year(dataset_metadata: dict):\n    \"\"\"\n    Get the publication year of the dataset from the dataset metadata.\n    Args:\n        dataset_metadata: The dataset metadata dictionary.\n\n    Returns:\n        str: The publication year of the dataset.\n    \"\"\"\n\n    # Check if the dataset is a datacite dataset\n    is_datacite = 'datacite.creator' in dataset_metadata\n\n    # Check if the dataset has a 'datacite.publicationyear' key. If it does, use that as the publication year.\n    if is_datacite:\n        # If the publication year is a string, return the first four characters of the string.\n        if isinstance(dataset_metadata['datacite.publicationyear'], str):\n            return dataset_metadata['datacite.publicationyear'][:4]\n        # If the publication year is a list, return the first four characters of the first element in the list since the second element is empty\n        else:\n            return dataset_metadata['datacite.publicationyear'][0][:4]\n\n    # If the dataset does not have a 'datacite.publicationyear' key, use the 'publicationYear' key.\n    else:\n        # If the publication year is a string, return the publication year.\n        if isinstance(dataset_metadata['publicationYear'], str):\n            return dataset_metadata['publicationYear']\n        else:\n            return dataset_metadata['publicationYear'][0][:4]\n\n\ndef get_extras(dataset_metadata: dict):\n    \"\"\"\n    Get the extras list for the dataset from the dataset metadata.\n    These are the metadata fields that are not part of the main dataset metadata.\n    Args:\n        dataset_metadata: The dataset metadata dictionary.\n\n    Returns:\n        list: The extras list for the dataset.\n    \"\"\"\n\n    # Initialize the extras list\n    extras = []\n\n    # List of keys to exclude from the extras list\n    dont_include = ['title', 'description', 'creator', 'subject', 'rights', 'identifier', 'date_created',\n                    'date_modified', 'de_path', 'datacite.creator', 'datacite.title', 'datacite.publicationyear',\n                    'publicationYear', 'Creator', 'Title', 'Identifier', 'version']\n\n    # Add the citation to the extras list\n    extras.append({'key': 'Citation', 'value': create_citation(dataset_metadata)})\n    extras.append({'key': 'Date created in discovery environment', 'value': dataset_metadata['date_created']})\n    extras.append({'key': 'Date last modified in discovery environment', 'value': dataset_metadata['date_modified']})\n\n    # If any of the keys in the dataset metadata are not in the 'dont_include' list, add them to the extras list\n    # If the values are lists, join them with a comma\n    for key, value in dataset_metadata.items():\n        if key not in dont_include:\n            if isinstance(value, list):\n                extras.append({'key': key, 'value': ', '.join(value)})\n            else:\n                extras.append({'key': key, 'value': value})\n\n\n    return extras\n\n\ndef migrate_dataset_and_files(dataset_metadata: dict):\n    dataset_metadata = clean_dataset_metadata(dataset_metadata)\n    # pretty_print(dataset_metadata)\n    # print(\"\\n\\n\\n\")\n    #\n    # is_datacite = 'datacite.creator' in dataset_metadata\n    # print(f\"Is DataCite: {is_datacite}\")\n\n\n    data = {\n        # 'name': get_title(dataset_metadata).lower().replace(' ', '-').replace('(', '').replace(')', ''),\n        # 'title': get_title(dataset_metadata),\n        # 'notes': dataset_metadata['description'],\n        'owner_org': 'tanmay-s-playground',\n        'private': False,\n        # 'author': dataset_metadata['creator'],\n        # 'author_email': 'john.doe@example.com', # DONT FORGET TO ADD EMAIL FOR THAT ONE DATASET\n        # 'tags': [{'name': 'example'}, {'name': 'dataset'}],\n        'groups': [\n            {\n                \"description\": \"All data that have been given a permanent identifier (DOI or ARK) by CyVerse. These data are stable and contents will not change.\",\n                \"display_name\": \"CyVerse Curated\",\n                \"id\": \"881288fa-e1bf-4ee8-8894-d97976043e4f\",\n                \"image_display_url\": \"\",\n                \"name\": \"cyverse-curated\",\n                \"title\": \"CyVerse Curated\"\n            }\n        ],\n        'extras': get_extras(dataset_metadata)\n    }\n\n    # Get the title of the dataset\n    title = get_title(dataset_metadata).strip()\n    ## Set the 'name' key to the title of the dataset with spaces replaced by hyphens and parentheses removed\n    data['name'] = title.lower().replace(' ', '-').replace('(', '').replace(')', '').replace('.', '-').replace('\"', '').replace('/', '-')\n    ## Set the 'title' key to the title of the dataset\n    data['title'] = title\n\n\n    # Set the 'notes' key to the description of the dataset depending on whether the description is stored in the 'description' key or 'Description' key\n    try:\n        data['notes'] = dataset_metadata['description']\n    except KeyError:\n        data['notes'] = dataset_metadata['Description']\n\n\n    # Set the 'author' key to the creator of the dataset\n    data['author'] = get_author(dataset_metadata)\n\n    # Set the keys for the license depending on the license specified in the dataset metadata\n    if \"ODC PDDL\" in dataset_metadata['rights']:\n        data['license_id'] = \"odc-pddl\"\n        data['license_title'] = \"Open Data Commons Public Domain Dedication and License (PDDL)\"\n        data['license_url'] = \"http://www.opendefinition.org/licenses/odc-pddl\"\n    elif \"CC0\" in dataset_metadata['rights']:\n        data['license_id'] = \"cc-zero\"\n        data['license_title'] = \"Creative Commons CCZero\"\n        data['license_url'] = \"http://www.opendefinition.org/licenses/cc-zero\"\n    else:\n        data['license_id'] = \"notspecified\"\n        data['license_title'] = \"License not specified\"\n\n    # If there is a 'subject' key in the dataset metadata, add it to the tags depending on whether its a string or a list\n    if 'subject' in dataset_metadata:\n        if isinstance(dataset_metadata['subject'], str):\n            subjects = dataset_metadata['subject'].split(',')\n            data['tags'] = [{'name': subject} for subject in subjects]\n        else:\n            data['tags'] = [{'name': subject} for subject in dataset_metadata['subject']]\n\n    # If there is a 'version' or 'Version' key in the dataset metadata, add it to the data dictionary\n    if 'version' in dataset_metadata:\n        data['version'] = dataset_metadata['version']\n    elif 'Version' in dataset_metadata:\n        data['version'] = dataset_metadata['Version']\n\n    # Print the data dictionary\n    # print(\"Data Dictionary: \")\n    # pretty_print(data)\n\n    # Create the dataset\n    dataset_response = ckan.create_dataset(data)\n    print(f'Dataset creation response: {dataset_response}')\n\n    # Get the dataset ID\n    dataset_id = dataset_response['result']['id']\n    print(f'Dataset ID: {dataset_id}')\n    print('\\n')\n\n\n    # --------------------------------- FILES ---------------------------------\n\n\n    # Get the list of files in the dataset\n    files = de.get_files(dataset_metadata['de_path'])\n    # print('\\n\\n\\nFiles without metadata: ')\n    # pretty_print(files)\n\n    # print(\"\\n\\n\\nFiles with Metadata: \\n\")\n    # print(\"Files\")\n    for file in files['files']:\n        file_metadata = de.get_all_metadata_file(file)\n        # pretty_print(file_metadata)\n\n        data = {\n            'package_id': dataset_id,\n            'name': file_metadata['file_name'],\n            'description': None,\n            'url': file_metadata['web_dav_location'],\n            'format': file_metadata['file_type'],\n            'Date created in discovery environment': file_metadata['date_created'],\n            'Date last modified in discovery environment': file_metadata['date_modified']\n        }\n        response = ckan.add_resource_link(data)\n        # print(f'Resource creation response: {response}')\n        # print(\"\\n\\n\")\n\n    # print(\"Folders\")\n    for folder in files['folders']:\n        folder_metadata = de.get_all_metadata_file(folder)\n        # pretty_print(folder_metadata)\n\n        data = {\n            'package_id': dataset_id,\n            'name': folder_metadata['file_name'],\n            'description': None,\n            'url': folder_metadata['web_dav_location'],\n            'format': 'folder',\n            'Date Created in Discovery Environment': folder_metadata['date_created'],\n            'Date Last Modified in Discovery Environment': folder_metadata['date_modified']\n        }\n        response = ckan.add_resource_link(data)\n        # print(f'Resource creation response: {response}')\n        #\n        # print(\"\\n\\n\")\n\ndef pretty_print(json_data):\n    \"\"\"\n    Format and print JSON data in a readable way.\n\n    Parameters:\n    json_data (dict): JSON data to be pretty-printed.\n    \"\"\"\n    print(json.dumps(json_data, indent=4, sort_keys=True))\n\nif __name__ == '__main__':\n\n    # ckan.delete_all_datasets_in_organization('tanmay-s-playground')\n\n    de_datasets: list[dict] = de.get_datasets()\n    # pretty_print(de_datasets)\n\n    # print(\"\\n\\n\\n\")\n    # ckan_datasets = ckan.list_datasets(group='cyverse-curated')\n    ckan_datasets = ckan.list_datasets(organization='tanmay-s-playground')\n    # for dataset in ckan_datasets:\n    #     pretty_print(dataset)\n    # print(\"\\n\\n\\n\")\n\n    count = 0\n\n    # Iterate through each dataset in the discovery environment to see if they exist in CKAN and whether they need to be updated\n    for de_dataset in de_datasets:\n        if 3 &lt; count &lt; 10:\n            # Get the metadata for the dataset in the discovery environment\n            de_dataset_metadata = de.get_all_metadata_dataset(de_dataset)\n            ## print(\"Discovery Environment Dataset Metadata\")\n            ## pretty_print(de_dataset_metadata)\n\n            # Get the title of the dataset in the discovery environment\n            de_dataset_title = get_title(de_dataset_metadata).strip()\n            # print(f\"Discovery Environment Dataset Title: {de_dataset_title}\")\n\n            # Iterate through each dataset in CKAN to see if the titles match\n            for ckan_dataset in ckan_datasets:\n                # pretty_print(ckan_dataset)\n                # Get the title of the dataset in CKAN\n                ckan_dataset_title = ckan_dataset['result']['title'].strip()\n                # print(f\"CKAN Dataset Title: {ckan_dataset_title}\")\n                if de_dataset_title == ckan_dataset_title:\n                    print(f\"Matched: {de_dataset_title}\")\n\n                    # Get the last modified date for the dataset in the discovery environment\n                    last_modified_de = de_dataset_metadata['date_modified']\n\n                    # Get the last modified date for the dataset in CKAN\n                    for extra in ckan_dataset['result']['extras']:\n                        if extra['key'] == 'Date last modified in discovery environment':\n                            last_modified_ckan = extra['value']\n\n                    # If the dataset in the discovery environment has been modified update the dataset in CKAN\n                    # by deleting the old dataset and creating a new one with the updated metadata and files\n                    if last_modified_de != last_modified_ckan:\n                        ckan_dataset_id = ckan_dataset['result']['id']\n                        print(\"Rewriting\")\n                        ckan.delete_dataset(ckan_dataset_id)\n                        migrate_dataset_and_files(de_dataset_metadata)\n\n                    # Break out of the loop if the dataset is found in CKAN\n                    break\n\n\n\n            # If the dataset does not exist in CKAN, create a new dataset\n            else:\n                print(f\"Creating New Dataset in CKAN: {de_dataset_title}\")\n                migrate_dataset_and_files(de_dataset_metadata)\n\n            print(\"\\n\")\n        count += 1\n</code></pre>"},{"location":"poster/","title":"Project Poster","text":""},{"location":"prompt_engineering_examples/","title":"Prompt Engineering Examples","text":"<p>Note: These examples are from Day 6 (6/10/24) of the logbook.</p>"},{"location":"prompt_engineering_examples/#example-1-internship-project-help","title":"Example 1: Internship Project Help","text":"<p>System Instructions:</p> <p>You are a highly knowledgeable and helpful assistant specializing in Linux, web page design, CKAN (Comprehensive Knowledge Archive Network), JSON, Python, and command line interface with APIs. You are also proficient in iRODS (Integrated Rule-Oriented Data System). Your task is to assist the user with technical queries, provide detailed explanations, and offer practical solutions or examples where applicable for their internship project. Here are details about the project so that you are familiar with it.</p> <p>Project Title:  Data Engineering to support customized data commons for research projects Project </p> <p>Details:  Large-scale data repositories that are hosted in a commercial cloud are common for NIH, NASA funded projects, and analysis performed with these data sets with private local data is a regular occurrence. Eventually, the final results have to be made public. Sharing these multiple data sources and analyses in a unified form is a challenge for most projects. I will create a novel protocol that will allow labs to create their own data commons that can be customized for their projects and meet federal data-sharing requirements.</p> <p>Ensure that your responses are clear, concise, and tailored to the user's level of understanding. Use a friendly and professional tone, and format your answers in a way that enhances readability, such as using bullet points, numbered lists, step-by-step instructions, or code blocks.</p> <p>Remember to ask clarifying questions if the user's request is ambiguous, and offer follow-up assistance to ensure they fully understand your response.</p> <p>User Prompt:</p> <p>Can you tell me how CKAN could be used in a project like this and how I can get started with it?</p> <p>Response:</p> <p>Absolutely, CKAN (Comprehensive Knowledge Archive Network) is an excellent choice for managing and sharing datasets, especially in a project like the one you're undertaking. Here's how CKAN might be utilized and how to get started with it:</p> <p>How CKAN Can Be Used in Your Project:</p> <ol> <li>Dataset Management:</li> <li>Cataloging and Metadata: CKAN allows for detailed cataloging of datasets with rich metadata, making it easier to manage and search through large repositories.</li> <li> <p>Versioning: Keep track of different versions of datasets.</p> </li> <li> <p>Search and Discovery:</p> </li> <li>Powerful Search Capabilities: CKAN provides a robust search functionality to find datasets based on various metadata fields.</li> <li> <p>Data Preview and Visualization: Users can preview datasets within CKAN and visualize data through built-in tools.</p> </li> <li> <p>Access Control:</p> </li> <li>User Management: Define roles and permissions to control who can view, add, and modify datasets.</li> <li> <p>Private and Public Datasets: Manage private datasets during research and make them public when ready for sharing.</p> </li> <li> <p>APIs and Integration:</p> </li> <li>API Access: CKAN provides a RESTful API, allowing for easy integration with other systems</li> </ol>"},{"location":"prompt_engineering_examples/#example-2-cricket-helper","title":"Example 2: Cricket Helper","text":"<p>System Instructions:</p> <p>You are a cricket expert knowledgeable about the rules, history, famous players, match strategies, and current events in cricket. Your goal is to provide comprehensive and insightful responses to user queries.</p> <p>Maintain an enthusiastic and informative tone in your responses. Ensure that your explanations are clear and engaging. </p> <p>Provide responses in detailed paragraphs for in-depth queries and use bullet points for quick facts or summaries. Use match data, player statistics, and historical references where relevant. </p> <p>Keep your responses concise but also informative. For complex queries or detailed analyses, provide more extensive explanations as needed. </p> <p>If the user has additional questions or needs clarification, invite them to ask follow-up questions. Offer to provide further insights or resources if necessary.</p> <p>User Prompt 1:</p> <p>What is the last tournament that you know of and what can you tell me about it?</p> <p>Response 1:</p> <p>As of my last update, the most recent significant cricket tournament is the ICC Men's T20 World Cup 2022, which took place in Australia from October 16 to November 13, 2022. Here are some detailed insights into the tournament:</p> <p>Overview:</p> <ul> <li>Host Country: Australia</li> <li>Tournament Format: The tournament included a first round, Super 12 group stage, semi-finals, and the final. Teams were divided into two groups of six, and the top teams from each group advanced to the knockout stages.</li> </ul> <p>Key Teams:</p> <ul> <li>Australia: As the defending champions and hosts, they were favorites and had a strong squad featuring players like Aaron Finch (captain), David Warner, and Pat Cummins.</li> <li>India: Known for their strong batting lineup with players like Rohit Sharma, Virat Kohli, and emerging star Suryakumar Yadav.</li> <li>England: A well-balanced side led by Eoin Morgan, with explosive batsmen like Jos Buttler and Jason Roy.</li> <li>Pakistan: Featuring a formidable pace attack led by Shaheen Afridi and a reliable batting order with Babar Azam and Mohammad Rizwan.</li> <li>New Zealand: Consistent performers in ICC tournaments, led by Kane Williamson, with a balanced team featuring players like Trent Boult and Devon Conway. Notable Matches:</li> <li></li> <li>India vs. Pakistan: A highly anticipated match that lived up to its billing. Pakistan successfully chased down a formidable target set by India, thanks to a brilliant opening partnership between Babar Azam and Mohammad Rizwan.</li> <li>England vs. New Zealand: A key match in the Super 12 stage, where England showcased their firepower, with standout performances from Jos Buttler and Adil Rashid leading them to victory.</li> <li>Australia vs. South Africa: An early Super 12 clash that set the tone for both teams, with Australia snatching a narrow win thanks to their potent bowling attack.</li> </ul> <p>Semi-Finals:</p> <ul> <li>England vs. New Zealand: A thrilling contest where New Zealand chased down England's total with some late heroics from Daryl Mitchell and Jimmy Neesham.</li> <li>Pakistan vs. Australia: Another nail-biter that saw Australia overcoming Pakistan\u2019s target in the last over, with crucial contributions from Matthew Wade and Marcus Stoinis.</li> </ul> <p>Final:</p> <ul> <li>New Zealand vs. Australia: Held at the Melbourne Cricket Ground, the final was a historic trans-Tasman clash. New Zealand batted first and posted a competitive score, but it was Australia's turn to shine. With a splendid innings from Mitchell Marsh and a good opening stand by David Warner, Australia chased down the target comfortably to win their first T20 World Cup.</li> </ul> <p>Key Players of the Tournament:</p> <ul> <li>David Warner (Australia): Awarded Player of the Tournament for his consistent performances with the bat.</li> <li>Babar Azam (Pakistan): The leading run-scorer of the tournament, showcasing his prowess as a top-order batsman.</li> <li>Adam Zampa (Australia): Consistently effective with the ball, playing a crucial role in Australia's success.</li> </ul> <p>The ICC Men's T20 World Cup 2022 was memorable for its competitive matches, standout individual performances, and the crowning of Australia as the champions, marking a significant achievement in their cricketing history. If you have any more specific queries about matches, players, or statistics from this tournament, feel free to ask!</p> <p>User Prompt 2:</p> <p>What can you tell me about the 2024 cricket world cup?</p> <p>Response 2:</p> <p>The ICC Men's T20 World Cup 2024 is an eagerly anticipated event in the cricket calendar. Here's some detailed information about the tournament:</p> <p>Overview:</p> <ul> <li>Host Countries: The 2024 T20 World Cup will be co-hosted by the United States and the West Indies. This is a historic move by the ICC, as it marks the first time that a major cricketing event will be held in the USA, signaling the sport's expansion into new markets.</li> <li>Dates: While the exact dates are yet to be officially confirmed, the tournament is expected to occur in the latter half of the year, typically between October and November.</li> </ul> <p>Format:</p> <ul> <li>The format will mirror previous editions, featuring:</li> <li>First Round: Lower-ranked teams compete for spots in the Super 12 stage.</li> <li>Super 12 Stage: Two groups of six teams each. The top teams from each group advance to the knockout stages.</li> <li>Semi-Finals: The top two teams from each Super 12 group will play in the semi-finals.</li> <li>Final: The winners of the semi-finals will clash in the final to crown the World Champions.</li> </ul> <p>Participating Teams:</p> <ul> <li>The tournament will feature 20 teams, an increase from previous editions. Qualified teams include traditional powerhouses like India, Australia, England, Pakistan, South Africa, and the co-hosts West Indies.</li> <li>Emerging teams and associates will also compete, providing a platform for the growth of cricket globally.</li> </ul> <p>Key Venues:</p> <ul> <li>USA Venues: Florida's Central Broward Regional Park, known for hosting several international fixtures, is likely to be a marquee venue. Efforts are underway to upgrade and add more cricket-specific facilities, possibly in states like Texas and California.</li> <li>West Indies Venues: Iconic cricket stadiums like Sabina Park in Jamaica, Kensington Oval in Barbados, and Queen's Park Oval in Trinidad and Tobago will likely host many games.</li> </ul> <p>Significance:</p> <ul> <li>Growth of Cricket: The decision to co-host in the USA and the West Indies reflects the ICC's strategy to globalize the sport. The USA, with a growing cricket fanbase, offers a unique opportunity to introduce the sport to new audiences.</li> </ul> <p>Preparations:</p> <ul> <li>Both the USA and West Indies\u2019 cricket boards have been actively preparing for the event. This includes upgrading infrastructure, enhancing training facilities, and organizing local tournaments to improve competitive standards.</li> </ul> <p>Challenges and Excitement:</p> <ul> <li>Logistical Coordination: Managing a tournament spread across two diverse regions presents challenges, but it also allows for a unique fan experience, combining the excitement of cricket in traditional and non-traditional territories.</li> <li>Fan Engagement: There is significant excitement about engaging the diaspora communities in the USA who are passionate about cricket and introducing the sport to new fans.</li> </ul> <p>Teams to Watch:</p> <ul> <li>West Indies: Known for their powerful T20 performances and having a home advantage.</li> <li>USA: An underdog but with potential and home conditions favoring them.</li> <li>Traditional Powerhouses: Teams like India, Australia, and England will inevitably be strong contenders.</li> </ul> <p>Conclusion:</p> <p>The 2024 ICC Men's T20 World Cup is set to be a landmark event, not just for its competitive cricket but for its potential to significantly boost the sport's profile in the USA. With an expanded roster of 20 teams, fans can expect a thrilling and diverse showcase of T20 cricket that will hopefully mark the beginning of a new era in the sport's global development.</p> <p>If you have more specific questions or need further details about any aspect of the 2024 T20 World Cup, feel free to ask!</p>"},{"location":"references/","title":"References","text":"<p>National Academies of Sciences, Engineering, and Medicine. (2018). Open science by design. In National Academies Press eBooks. https://doi.org/10.17226/25116</p> <p>National Institutes of Health. (n.d.). NIH Data Commons Pilot Phase. https://commonfund.nih.gov/commons</p> <p>Overview \u2014 CKAN 2.10.4 documentation. (n.d.). https://docs.ckan.org/en/2.10/</p> <p>Gradio Documentation (n.d.). https://www.gradio.app/docs</p>"}]}